{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa9c9c8e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d2ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "# from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from pytz import timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d97516",
   "metadata": {},
   "source": [
    "### Getting the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74428c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = timezone('EST')\n",
    "today = datetime.now(tz) \n",
    "\n",
    "# Making datetime the same format as the EGLE database\n",
    "today = today.strftime(\"%-m/%-d/%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41457829",
   "metadata": {},
   "source": [
    "### Getting a list of current sources\n",
    "List provided by EGLE in May 2022 via FOIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9fa501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sourceList = pd.read_csv('CMS-Subject-Sources-Simple.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4ff5b",
   "metadata": {},
   "source": [
    "### Looking for sources in this list that were updated today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba76027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling up the EGLE database\n",
    "raw_html = requests.get(\"https://www.egle.state.mi.us/aps/downloads/SRN/\", verify=False).content\n",
    "doc = BeautifulSoup(raw_html, \"html.parser\")\n",
    "text = doc.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the source name and date the directory was updated\n",
    "# Although the text shows the source ID next to the date, \n",
    "# the date actually appears before the source ID on the website\n",
    "# So my regex is looking for the source ID ~after~ the date.\n",
    "sourceDates = re.findall(r\"(\\d\\d?/\\d\\d?/\\d{4})\\s+\\d+:\\d{2}\\s[A-Z]{2}\\s*<dir>\\s([A-Z]\\d{4})\",text)\n",
    "sourceDatesUnknown = re.findall(r\"(\\d\\d?/\\d\\d?/\\d{4})\\s+\\d+:\\d{2}\\s[A-Z]{2}\\s*<dir>\\s([U]\\d{9})\",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f61df5",
   "metadata": {},
   "source": [
    "### Making a list of directory URLs that have had updates today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cddf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "updates = []\n",
    "sourcesUpdated = []\n",
    "\n",
    "sourceListSRNs = sourceList.srn.to_list()\n",
    "\n",
    "for source in sourceDates:\n",
    "    sourceID = source[1]\n",
    "    date = source[0]\n",
    "    if (date == today) & (sourceID in sourceListSRNs):\n",
    "        link = \"https://www.egle.state.mi.us/aps/downloads/SRN/\"+ sourceID\n",
    "        updates.append(link)\n",
    "        sourcesUpdated.append(sourceID)\n",
    "        \n",
    "for source in sourceDatesUnknown:\n",
    "    if (date == today) & (sourceID in sourceList.srn):\n",
    "        link = \"https://www.egle.state.mi.us/aps/downloads/SRN/\"+ sourceID\n",
    "        updates.append(link)\n",
    "        sourcesUpdated.append(sourceID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0fcac",
   "metadata": {},
   "source": [
    "### Getting the current document datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aab37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents I already have\n",
    "oldDocs = pd.read_csv(\"output/EGLE-AQD-document-dataset-full.csv\")\n",
    "\n",
    "# list of URLs\n",
    "oldDocs = oldDocs.doc_url.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA documents that didn't fit the regex\n",
    "oldExtras = pd.read_csv(\"output/EGLE-AQD-extra-documents.csv\")\n",
    "\n",
    "# list of URLs\n",
    "oldExtras = oldExtras.doc_url.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697d9b8",
   "metadata": {},
   "source": [
    "### Looking for new documents in the updated directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d768914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allSourcesData = []\n",
    "allSourcesExtras = []\n",
    "mistakes = []\n",
    "\n",
    "# Look in the directories that have updates\n",
    "for directory in updates:\n",
    "    raw_html = requests.get(directory, verify=False).content\n",
    "    doc = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    links = doc.find_all('a')\n",
    "    sourceData = []\n",
    "    sourceExtras = []\n",
    "    # For each directory, look at the urls\n",
    "    for link in links:\n",
    "        data = {}\n",
    "        other = {}\n",
    "        doc_url = 'https://www.egle.state.mi.us'+link['href']\n",
    "        \n",
    "        # I only want new URLs. Also, don't capture the ['To Parent Directory'] link\n",
    "        if (doc_url not in oldDocs) & (doc_url != 'https://www.egle.state.mi.us/aps/downloads/SRN/'):\n",
    "            \n",
    "            # Save data from documents that fit the regex\n",
    "            try:\n",
    "                # Source_ID\n",
    "                data['source_id'] = re.findall(r\"SRN/(.*)\",directory)[0]\n",
    "                # Document code\n",
    "                data['doc_type'] = re.findall(r\"_?([A-Z]+\\d?\\d?)_\",link.text, re.IGNORECASE)[0]\n",
    "                # Date\n",
    "                data['date'] = re.findall(r\"_(\\d{8})\", link.text)[0]\n",
    "                # URL\n",
    "                data['doc_url'] = doc_url\n",
    "                sourceData.append(data)\n",
    "\n",
    "            # Save links that don't fit the regex or just don't work for some reason (misakes)\n",
    "            except:\n",
    "                try:\n",
    "                    # Source_ID\n",
    "                    other['source_id'] = re.findall(r\"SRN/(.*)\", directory)[0]\n",
    "\n",
    "                    # extra doc names that don't fit the regex\n",
    "                    other['doc_name'] = link.text\n",
    "\n",
    "                    # extra doc URLs\n",
    "                    other['doc_url'] = doc_url\n",
    "                    if (other['doc_name'] != '[To Parent Directory]') & (doc_url not in oldExtras):\n",
    "                        sourceExtras.append(other)\n",
    "\n",
    "\n",
    "                except:\n",
    "                    # If there are still links that don't work, save them in a list\n",
    "                    mistake = link\n",
    "                    mistakes.append(mistake)\n",
    "\n",
    "    if len(sourceData) != 0:\n",
    "        allSourcesData.append(sourceData)\n",
    "    if len(sourceExtras) != 0:\n",
    "        allSourcesExtras.append(sourceExtras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9397698",
   "metadata": {},
   "source": [
    "### Formatting new document data and adding it to existing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca7cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(allSourcesData) != 0:\n",
    "    \n",
    "    # Converting new data into a dataframe\n",
    "    dfList = [pd.DataFrame(oneList) for oneList in allSourcesData]\n",
    "    newDocs = pd.concat(dfList, ignore_index=True)\n",
    "    newDocsURLs = newDocs.doc_url.to_list()\n",
    "    \n",
    "    # Converting to datetime and adding year column\n",
    "    newDocs.date = pd.to_datetime(newDocs.date,format='%Y%m%d',errors='coerce')\n",
    "    newDocs['year'] = newDocs.date.dt.year\n",
    "    \n",
    "    # Removing leading zeroes\n",
    "    newDocs.loc[newDocs.doc_type.str.contains('0\\d'),'doc_type'] = newDocs.loc[newDocs.doc_type.str.contains('0\\d'),'doc_type'].str.replace(\"0\",'')\n",
    "\n",
    "    # Making codes all uppercase\n",
    "    newDocs.doc_type = newDocs.doc_type.str.upper()\n",
    "\n",
    "    # Fixing a common transposition of \"RVN\"\n",
    "    newDocs.doc_type = newDocs.doc_type.str.replace('VNR','RVN')\n",
    "    \n",
    "    # Reading in a key of document code\n",
    "    ## Read about the creation of the key at srjouppi.github.io\n",
    "    key = pd.read_csv('EGLE-AQD-document-code-key.csv')\n",
    "\n",
    "    # Merging with document code key \n",
    "    newDocs = newDocs.merge(key)\n",
    "\n",
    "    # Merging with source list for identifying information\n",
    "    newDocs = newDocs.merge(sourceList,how='left', left_on='source_id',right_on='srn')\n",
    "\n",
    "    # Rearranging Columns\n",
    "    newDocs = newDocs[['date','year','facility_name','doc_type','type_name','doc_url','srn','epa_class','address_line1','city','zip','county','egle_district','staff','type_simple','type_name_simple','address_full']]\n",
    "    \n",
    "    # Reading in existing document dataset\n",
    "    oldDocs = pd.read_csv(\"output/EGLE-AQD-document-dataset-full.csv\", dtype={'zip_cd':str})\n",
    "    oldDocs.date = pd.to_datetime(oldDocs.date)\n",
    "    \n",
    "    # Adding new documents to dataset\n",
    "    allDocs = pd.concat([oldDocs, newDocs], axis=0,ignore_index=True)\n",
    "    \n",
    "    # Sorting by date\n",
    "    allDocs = allDocs.sort_values('date',ascending=False, ignore_index=True)\n",
    "    \n",
    "    # Overwriting document dataset file\n",
    "    allDocs.to_csv('output/EGLE-AQD-document-dataset-full.csv',index=False)\n",
    "    \n",
    "    # Saving a subset of the most current documents (last 90 days) that is easier to use \n",
    "    today = pd.to_datetime(today)\n",
    "    \n",
    "    # Getting a duration from the date of the file to today's date\n",
    "    def duration(date):\n",
    "        return today - date\n",
    "\n",
    "    allDocs['duration'] = allDocs.date.apply(duration)\n",
    "    allDocs.duration = allDocs.duration.dt.days\n",
    "    \n",
    "    # If the duration is less than 91, save it to the 90-day file.\n",
    "    allDocs.query('duration < 91').drop(['duration'],axis=1).sort_values('date',ascending=False).to_csv('output/EGLE-AQD-document-dataset-90days.csv',index=False)\n",
    "    \n",
    "else:\n",
    "    newDocsURLs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e7edb1",
   "metadata": {},
   "source": [
    "### Adding new extra documents to existing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in my csv of extra documents\n",
    "oldExtras = pd.read_csv(\"output/EGLE-AQD-extra-documents.csv\")\n",
    "\n",
    "# Turning my list of lists of dicts of Extra Documents into a dataframe\n",
    "if len(allSourcesExtras) != 0:\n",
    "    dfList = [pd.DataFrame(oneList) for oneList in allSourcesExtras]\n",
    "    newExtras = pd.concat(dfList, ignore_index=True)\n",
    "    newExtrasURLs = newExtras.doc_url.to_list()\n",
    "    \n",
    "    # Adding my new documents to my old documents\n",
    "    allExtras = pd.concat([oldExtras ,newExtras], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Overwriting the old csv with updates\n",
    "    allExtras.to_csv(\"output/EGLE-AQD-extra-documents.csv\", index=False)\n",
    "    \n",
    "else:\n",
    "    newExtrasURLs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73878d9",
   "metadata": {},
   "source": [
    "### Creating today's scrape report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c80bc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading in my most recent scrape report\n",
    "oldReport = pd.read_csv(\"output/EGLE-AQD-scraper-report.csv\")\n",
    "oldReport.date = pd.to_datetime(oldReport.date)\n",
    "\n",
    "scrapeReport = []\n",
    "data = {}\n",
    "\n",
    "data['date'] = today\n",
    "\n",
    "data['sources_updated'] = len(sourcesUpdated)\n",
    "\n",
    "data['docs_found'] = len(newDocsURLs)\n",
    "\n",
    "# All \"Type Simple\" Doc Types:\n",
    "docTypes = ['SAR',\n",
    " 'FCE',\n",
    " 'TEST',\n",
    " 'VN',\n",
    " 'RVN',\n",
    " 'ACO',\n",
    " 'ENFN',\n",
    " 'STIP',\n",
    " 'CJ',\n",
    " 'ASBVN',\n",
    " 'AFO',\n",
    " 'RASBVN',\n",
    " 'CD']\n",
    "\n",
    "# If documents found today, getting counts for the different types:\n",
    "if data['docs_found'] != 0:\n",
    "    newDocsTypes = newDocs.type_simple.value_counts().to_dict()\n",
    "else:\n",
    "    newDocsTypes = []\n",
    "\n",
    "# For each document type, look to see if it was found today,\n",
    "# If so, add the count. If not, return \"None\"\n",
    "for docType in docTypes:\n",
    "    if docType in newDocsTypes:\n",
    "        data[docType] = newDocsTypes[docType]\n",
    "    else:\n",
    "        data[docType] = None\n",
    "\n",
    "data['extras_found'] = len(newExtrasURLs)\n",
    "\n",
    "data['mistakes_found'] = len(mistakes)\n",
    "    \n",
    "if data['mistakes_found'] != 0:\n",
    "    \n",
    "    data['mistakes'] = mistakes\n",
    "else:\n",
    "    data['mistakes'] = None\n",
    "\n",
    "scrapeReport.append(data)\n",
    "\n",
    "newReport = pd.DataFrame(scrapeReport)\n",
    "\n",
    "# Adding the new report to the old reports\n",
    "newReport = pd.concat([oldReport,newReport], axis=0, ignore_index=True).sort_values('date',ascending=False)\n",
    "\n",
    "# Overwriting the report csv with update\n",
    "newReport.to_csv(\"output/EGLE-AQD-scraper-report.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('3.10.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "af45f0f9cd4a28cfcf29183ca144b092989a5259a776b218e39802bdf830046e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
