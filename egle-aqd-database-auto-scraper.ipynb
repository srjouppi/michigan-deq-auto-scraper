{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53089f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from pytz import timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74428c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What day is it?\n",
    "tz = timezone('EST')\n",
    "today = datetime.now(tz) \n",
    "\n",
    "# Making datetime the same format as the EGLE database\n",
    "today = today.strftime(\"%-m/%-d/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9fa501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading in the EGLE master list of sources to search database for known sources\n",
    "source_list_df = pd.read_csv(\"EGLE-AQD-source-list.csv\")\n",
    "\n",
    "# Getting a list of known sources\n",
    "source_id_list = source_list_df.id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba76027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the EGLE database home page and getting the text\n",
    "raw_html = requests.get(\"https://www.deq.state.mi.us/aps/downloads/SRN/\").content\n",
    "doc = BeautifulSoup(raw_html, \"html.parser\")\n",
    "text = doc.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the source name and date the directory was updated\n",
    "# Although the text shows the source ID next to the date, \n",
    "# the date actually appears before the source ID on the website\n",
    "# So my regex is looking for the source ID ~after~ the date.\n",
    "source_dates = re.findall(r\"(\\d\\d?/\\d\\d?/\\d{4})\\s+\\d+:\\d{2}\\s[A-Z]{2}\\s*<dir>\\s([A-Z]\\d{4})\",text)\n",
    "unknown_source_dates = re.findall(r\"(\\d\\d?/\\d\\d?/\\d{4})\\s+\\d+:\\d{2}\\s[A-Z]{2}\\s*<dir>\\s([U]\\d{9})\",text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cddf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a list of directory URLs that have had updates today\n",
    "updates = []\n",
    "sources_updated = []\n",
    "for source in source_dates:\n",
    "    source_id = source[1]\n",
    "    date = source[0]\n",
    "    if (date == today) & (source_id in source_id_list):\n",
    "        link = \"https://www.deq.state.mi.us/aps/downloads/SRN/\"+source_id\n",
    "        updates.append(link)\n",
    "        sources_updated.append(source_id)\n",
    "for source in unknown_source_dates:\n",
    "    if (date == today) & (source_id in source_id_list):\n",
    "        link = \"https://www.deq.state.mi.us/aps/downloads/SRN/\"+source_id\n",
    "        updates.append(link)\n",
    "        sources_updated.append(source_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aab37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the most recent csv of extra documents\n",
    "df = pd.read_csv(\"output/EGLE-AQD-extra-documents.csv\")\n",
    "\n",
    "# Getting a list of extra document urls I already have\n",
    "extra_doc_url_list = df.doc_url.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the most recent csv of documents\n",
    "df = pd.read_csv(\"output/EGLE-AQD-documents.csv\")\n",
    "\n",
    "# Getting a list of document urls I already have\n",
    "doc_url_list = df.doc_url.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d768914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scrape the directories that have had updates looking for urls that are not already in my csv\n",
    "all_sources_data = []\n",
    "all_sources_extras = []\n",
    "mistakes = []\n",
    "\n",
    "# Look in the directories that have updates\n",
    "for directory in tqdm(updates):\n",
    "    raw_html = requests.get(directory).content\n",
    "    doc = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    links = doc.find_all('a')\n",
    "    source_data = []\n",
    "    source_extras = []\n",
    "    \n",
    "    # For each directory, look at the urls\n",
    "    for link in links:\n",
    "        data = {}\n",
    "        other = {}\n",
    "        doc_url = 'https://www.deq.state.mi.us'+link['href']\n",
    "        \n",
    "        # I only want new URLs. Also, don't capture the ['To Parent Directory'] link\n",
    "        if (doc_url not in doc_url_list) & (doc_url != 'https://www.deq.state.mi.us/aps/downloads/SRN/'):\n",
    "            \n",
    "            # Save data from documents that fit the regex\n",
    "            try:\n",
    "                # Source_ID\n",
    "                data['source_id'] = re.findall(r\"^\\w\\w?\\d+\",link.text)[0]\n",
    "                # Document code\n",
    "                data['doc_type'] = re.findall(r\"_?([A-Z]+\\d?\\d?)_\",link.text, re.IGNORECASE)[0]\n",
    "                # Date\n",
    "                data['date'] = re.findall(r\"_(\\d{8})\", link.text)[0]\n",
    "                # URL\n",
    "                data['doc_url'] = doc_url\n",
    "                source_data.append(data)\n",
    "            \n",
    "            # Save links that don't fit the regex or just don't work for some reason (misakes)\n",
    "            except:\n",
    "                try:\n",
    "                    # Source_ID\n",
    "                    other['source_id'] = re.findall(r\"\\w\\w?\\d+\", directory)[0]\n",
    "                    \n",
    "                    # extra doc names that don't fit the regex\n",
    "                    other['doc_name'] = link.text\n",
    "                    \n",
    "                    # extra doc URLs\n",
    "                    other['doc_url'] = doc_url\n",
    "                    if (other['doc_name'] != '[To Parent Directory]') & (doc_url not in extra_doc_url_list):\n",
    "                        source_extras.append(other)\n",
    "                        \n",
    "                    \n",
    "                except:\n",
    "                    # If there are still links that don't work, save them in a list\n",
    "                    mistake = link\n",
    "                    mistakes.append(mistake)\n",
    "                    \n",
    "    if len(source_data) != 0:\n",
    "        all_sources_data.append(source_data)\n",
    "    if len(source_extras) != 0:\n",
    "        all_sources_extras.append(source_extras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca7cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning my list of lists of dicts of Source Data into a dataframe\n",
    "if len(all_sources_data) != 0:\n",
    "    list_of_dfs = [pd.DataFrame(one_list) for one_list in all_sources_data]\n",
    "    new_data_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "    new_data_urls = new_data_df.doc_url.to_list()\n",
    "    \n",
    "    # Adding my new documents to my old documents\n",
    "    updated_df = pd.concat([df,new_data_df], axis=0,ignore_index=True)\n",
    "    \n",
    "    # Overwriting the old csv with updates\n",
    "    updated_df.to_csv(\"output/EGLE-AQD-documents.csv\", index=False)\n",
    "    \n",
    "else:\n",
    "    new_data_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging documents with MDEQ Source Directory\n",
    "# To get identifying information\n",
    "\n",
    "if len(all_sources_data) != 0:\n",
    "    df = updated_df.merge(source_list_df, left_on=\"source_id\", right_on=\"id\", how=\"left\")\n",
    "    df = df.drop(['id'], axis=1)\n",
    "    df['date'] = pd.to_datetime(df['date'], format=\"%Y%m%d\", errors='coerce')\n",
    "    df['zip_code'] = df['zip_code'].astype(str).str[:5]\n",
    "    df = df[['name', 'doc_type','date','zip_code','county','full_address','source_id','geometry', 'doc_url']]\n",
    "    df.to_csv(\"output/MDEQ-SRN-documents-source-info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in my csv of extra documents\n",
    "df = pd.read_csv(\"output/EGLE-AQD-extra-documents.csv\")\n",
    "\n",
    "# Turning my list of lists of dicts of Extra Documents into a dataframe\n",
    "if len(all_sources_extras) != 0:\n",
    "    list_of_dfs = [pd.DataFrame(one_list) for one_list in all_sources_extras]\n",
    "    new_extras_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "    new_extras_urls = new_extras_df.doc_url.to_list()\n",
    "    \n",
    "    # Adding my new documents to my old documents\n",
    "    updated_df = pd.concat([df,new_extras_df], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Overwriting the old csv with updates\n",
    "    updated_df.to_csv(\"output/EGLE-AQD-extra-documents.csv\", index=False)\n",
    "    \n",
    "else:\n",
    "    new_extras_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c80bc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading in my most recent scrape report\n",
    "df = pd.read_csv(\"output/EGLE-AQD-scraper-report.csv\")\n",
    "\n",
    "# Creating today's scrape report\n",
    "\n",
    "scrape_report = []\n",
    "data = {}\n",
    "\n",
    "data['date'] = today\n",
    "\n",
    "data['updates_found'] = len(sources_updated)\n",
    "\n",
    "data['source_data'] = len(new_data_urls)\n",
    "    \n",
    "data['source_extras'] = len(new_extras_urls)\n",
    "\n",
    "data['mistakes'] = len(mistakes)\n",
    "\n",
    "if data['updates_found'] != 0:\n",
    "    data['sources_updated'] = sources_updated\n",
    "else:\n",
    "    data['sources_updated'] = None\n",
    "\n",
    "if data['source_data'] != 0:\n",
    "    data['source_data_urls'] = new_data_urls\n",
    "else: \n",
    "    data['source_data_urls'] = None\n",
    "    \n",
    "if data['source_extras'] != 0:\n",
    "    data['source_extras_urls'] = new_extras_urls\n",
    "else:\n",
    "    data['source_extras_urls'] = None\n",
    "    \n",
    "if data['mistakes'] != 0:\n",
    "    data['mistakes_urls'] = mistakes\n",
    "else:\n",
    "    data['mistakes_urls'] = None\n",
    "    \n",
    "scrape_report.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ea860",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = pd.DataFrame(scrape_report)\n",
    "\n",
    "# Adding the new report to the old reports\n",
    "report_df = pd.concat([df,report_df], axis=0, ignore_index=True)\n",
    "\n",
    "# Overwriting the report csv with update\n",
    "report_df.to_csv(\"output/EGLE-AQD-scraper-report.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
